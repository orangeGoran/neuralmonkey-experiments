; Same as bahdanau alpha but with parameter sizes mathcing the paper

[main]
; The main block contains the mandatory fields for running and experiment.
name="translation"
tf_manager=<tf_manager>
output="exp-nm-mt/out-example-translation_slo_ang"
batch_size=60
epochs=100
train_dataset=<train_data>
val_dataset=<val_data>
trainer=<trainer>
runners=[<runner_greedy>]
evaluation=[("target_greedy", "target", <bleu>)]
logging_period=80
validation_period=2000
random_seed=666

[tf_manager]
class=tf_manager.TensorFlowManager
num_threads=16
num_sessions=1

[bpe_preprocess]
class=processors.bpe.BPEPreprocessor
merge_file="exp-nm-mt/data/merge_file.bpe"

[bpe_postprocess]
class=processors.bpe.BPEPostprocessor

[bleu]
class=evaluators.bleu.BLEUEvaluator

[train_data]
; This is a definition of the training data object. Dataset is not a standard
; class, it treats the __init__ method's arguments as a dictionary, therefore
; the data series names can be any string, prefixed with "s_". To specify the
; output file for a series, use "s_" prefix and "_out" suffix, e.g.
; "s_target_out"
class=dataset.load_dataset_from_files
s_target="exp-nm-mt/data/train/Batch1a_en.txt"
s_source="exp-nm-mt/data/train/Batch1a_sl.txt"
preprocessors=[("source", "source_bpe", <bpe_preprocess>), ("target", "target_bpe", <bpe_preprocess>)]
lazy=True

[val_data]
; Validation data, the languages are not necessary here, encoders and decoders
; access the data series via the string identifiers defined here.
class=dataset.load_dataset_from_files
s_target="exp-nm-mt/data/dev/Batch2a_en.txt"
s_source="exp-nm-mt/data/dev/Batch2a_sl.txt"
preprocessors=[("source", "source_bpe", <bpe_preprocess>), ("target", "target_bpe", <bpe_preprocess>)]

[shared_vocabulary]
class=vocabulary.from_bpe
path="exp-nm-mt/data/merge_file.bpe"

[attention]
class=attention.Attention
name="att_sent_enc"
encoder=<encoder>
state_size=600
dropout_keep_prob=0.8

[encoder]
class=encoders.recurrent.SentenceEncoder
name="encoder"
rnn_size=600
max_input_len=50
embedding_size=600
dropout_keep_prob=1.0
data_id="source_bpe"
vocabulary=<shared_vocabulary>

[decoder]
class=decoders.decoder.Decoder
name="decoder"
encoders=[<encoder>]
attentions=[<attention>]
rnn_size=600
embedding_size=600
dropout_keep_prob=1.0
data_id="target_bpe"
max_output_len=50
vocabulary=<shared_vocabulary>
conditional_gru=True

[trainer]
; This block just fills the arguments of the trainer __init__ method.
class=trainers.cross_entropy_trainer.CrossEntropyTrainer
decoders=[<decoder>]
l2_weight=1.0e-08
clip_norm=1.0

[runner_greedy]
class=runners.runner.GreedyRunner
output_series="target_greedy"
decoder=<decoder>
postprocess=<bpe_postprocess>
